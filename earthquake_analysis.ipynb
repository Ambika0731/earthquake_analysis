{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp, udf, concat, lit, when\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/22 08:19:20 WARN Utils: Your hostname, Ambikas-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.4 instead (on interface en0)\n",
      "24/02/22 08:19:20 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/22 08:19:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Earthquake Analysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset into a DataFrame\n",
    "df = spark.read.csv(\n",
    "    \"database.csv\",\n",
    "    header=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Convert the Date and Time columns into a timestamp column named Timestamp\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"Timestamp_ISO\", to_timestamp(col(\"Date\"), \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\")\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"Timestamp_nonISO\",\n",
    "    to_timestamp(concat(col(\"Date\"), lit(\" \"), col(\"Time\")), \"MM/dd/yyyy HH:mm:ss\"),\n",
    ")\n",
    "\n",
    "df = df.select(\n",
    "    when(col(\"Timestamp_ISO\").isNull(), col(\"Timestamp_nonISO\"))\n",
    "    .otherwise(col(\"Timestamp_ISO\"))\n",
    "    .alias(\"Timestamp\"),\n",
    "    \"Date\",\n",
    "    \"Time\",\n",
    "    \"Latitude\",\n",
    "    \"Longitude\",\n",
    "    \"Type\",\n",
    "    \"Depth\",\n",
    "    \"Depth Error\",\n",
    "    \"Depth Seismic Stations\",\n",
    "    \"Magnitude\",\n",
    "    \"Magnitude Type\",\n",
    "    \"Magnitude Error\",\n",
    "    \"Magnitude Seismic Stations\",\n",
    "    \"Azimuthal Gap\",\n",
    "    \"Horizontal Distance\",\n",
    "    \"Horizontal Error\",\n",
    "    \"Root Mean Square\",\n",
    "    \"ID\",\n",
    "    \"Source\",\n",
    "    \"Location Source\",\n",
    "    \"Magnitude Source\",\n",
    "    \"Status\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Filter the dataset to include only earthquakes with a magnitude greater than 5.0\n",
    "df_filtered = df.filter(col(\"Magnitude\") > 5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Calculate the average depth and magnitude of earthquakes for each earthquake type\n",
    "avg_depth_magnitude = df_filtered.groupBy(\"Type\").agg(\n",
    "    {\"Depth\": \"avg\", \"Magnitude\": \"avg\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5: Implement a UDF to categorize the earthquakes into levels (e.g., Low, Moderate, High) based on their magnitudes\n",
    "def categorize_magnitude(magnitude):\n",
    "    if magnitude < 6.0:\n",
    "        return \"Low\"\n",
    "    elif magnitude < 7.0:\n",
    "        return \"Moderate\"\n",
    "    else:\n",
    "        return \"High\"\n",
    "\n",
    "\n",
    "categorize_magnitude_udf = udf(categorize_magnitude, StringType())\n",
    "df_filtered = df_filtered.withColumn(\n",
    "    \"MagnitudeLevel\", categorize_magnitude_udf(col(\"Magnitude\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6: Calculate the distance of each earthquake from a reference location (e.g., (0, 0))\n",
    "# Assuming reference location is (0, 0)\n",
    "\n",
    "\n",
    "def calculate_distance(lat, lon):\n",
    "    return ((lat - 0) ** 2 + (lon - 0) ** 2) ** 0.5\n",
    "\n",
    "\n",
    "calculate_distance_udf = udf(calculate_distance)\n",
    "df_filtered = df_filtered.withColumn(\n",
    "    \"DistanceFromReference\", calculate_distance_udf(col(\"Latitude\"), col(\"Longitude\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 7: Save the final DataFrame as a CSV file\n",
    "# Write CSV file with column header (column names)\n",
    "df.write.option(\"header\", True).csv(\n",
    "    \"final_analysis_data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
